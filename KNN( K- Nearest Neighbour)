# Import Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Import the dataset

dataset = pd.read_csv("Social_Network_Ads.csv")
X = dataset.iloc[:,[2,3]].values one. 
Y = dataset.iloc[:,4].values

-------------------------------------------------------------------------------------------------------------------------------------

#Test_Train spliting

from sklearn.cross_validation import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

-------------------------------------------------------------------------------------------------------------------------------------

#Taking care of categorical variable/ Feature Scaling 

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
-------------------------------------------------------------------------------------------------------------------------------------

#Fitting classifier to the Training Set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p= 2) 
classifier.fit(X_train, Y_train)

#metric / Distance Metric object = minkowski and p =2 is used for STANDARD EUCLIDIANS DISTANCE
#metric / Distance Metric object = minkowski and p =1 is used for MANHATTAN DISTANCE
#To inspect any element - select and press i

#Predicting the Test set results
Y_pred = classifier.predict(X_test)

#Making the confusion Matrix
#It will contain the predictions (Correct as well as Incorrect ones)

from sklearn.metrics import confusion_matrix   #Here confusion matrix is a function bcz class always starts with CAPITAL LETTER
cm = confusion_matrix(Y_test, Y_pred)

#[1] cm>
#     [69, 3
#      4, 24]
      
#Correct Pred = 69+24
#Incorrect Pred = 3+4 = 7 
# Much efficient than Logistic Regression
-------------------------------------------------------------------------------------------------------------------------------------

#Visualizing the training set

from matplotlib.colors import ListedColormap
X_set, Y_set = X_train, Y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min()-1, stop = X_set[:,0].max()+1 , step =0.01),
                     np.arange(start = X_set[:,1].min()-1, stop = X_set[:,1].max()+1 , step =0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red','green')) )
             
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(Y_set)):
       plt.scatter( X_set[Y_set == j, 0], X_set[Y_set == j, 1], c=ListedColormap(('red','green'))(i), label=j )
plt.title('KNN (Training Set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

-------------------------------------------------------------------------------------------------------------------------------------

#Visualizing the test set

from matplotlib.colors import ListedColormap
X_set, Y_set = X_test, Y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min()-1, stop = X_set[:,0].max()+1 , step =0.01),
                     np.arange(start = X_set[:,1].min()-1, stop = X_set[:,1].max()+1 , step =0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red','green')) )
             
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(Y_set)):
       plt.scatter( X_set[Y_set == j, 0], X_set[Y_set == j, 1], c=ListedColormap(('red','green'))(i), label=j )
plt.title('KNN (Test Set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()


